}
}
}
# ---------- 2. Benchmarking Function ----------
full_crossed_benchmark_bias_rho <- function(n_values, k_values, rho_conditions, reps = 10, eps = 0.05) {
results <- list()
for (n in n_values) {
for (k in k_values) {
for (rho_label in names(rho_conditions)) {
rho_val <- rho_conditions[[rho_label]]
Sigma <- matrix(rho_val, k, k)
diag(Sigma) <- 1
ic_times <- numeric(reps)
flexic_times <- numeric(reps)
ic_biases <- numeric(reps)
flexic_biases <- numeric(reps)
for (i in 1:reps) {
set.seed(i + n + k)
weird_data <- generate_weird_marginals(n, k)
# IC
start_ic <- Sys.time()
ic_data <- true_ic(weird_data, Sigma)
end_ic <- Sys.time()
ic_times[i] <- as.numeric(difftime(end_ic, start_ic, units = "secs"))
# Calculate IC bias
ic_spearman <- cor(ic_data, method = "spearman")[upper.tri(Sigma)]
target_S <- (6 / pi) * asin(Sigma[1, 2] / 2)
ic_rel_bias <- (ic_spearman - target_S) / target_S
ic_biases[i] <- mean(abs(ic_rel_bias))
# FLEXIC
start_flexic <- Sys.time()
flexic_data <- flex_ic(weird_data, Sigma, eps = eps)
end_flexic <- Sys.time()
flexic_times[i] <- as.numeric(difftime(end_flexic, start_flexic, units = "secs"))
# Calculate FLEXIC bias
flexic_spearman <- cor(flexic_data, method = "spearman")[upper.tri(Sigma)]
flexic_rel_bias <- (flexic_spearman - target_S) / target_S
flexic_biases[i] <- mean(abs(flexic_rel_bias))
}
median_ic <- median(ic_times)
median_flexic <- median(flexic_times)
mean_ic <- mean(ic_times)
mean_flexic <- mean(flexic_times)
speedup_median <- median_ic / median_flexic
speedup_mean <- mean_ic / mean_flexic
mean_abs_bias_ic <- mean(ic_biases)
mean_abs_bias_flexic <- mean(flexic_biases)
var_abs_bias_ic <- var(ic_biases)
var_abs_bias_flexic <- var(flexic_biases)
results[[paste0("n", n, "_k", k, "_rho", rho_label)]] <- data.frame(
n = n,
k = k,
rho_label = rho_label,
rho_value = rho_val,
median_time_IC_sec = median_ic,
median_time_flexIC_sec = median_flexic,
speedup_median = round(speedup_median, 2),
mean_time_IC_sec = mean_ic,
mean_time_flexIC_sec = mean_flexic,
speedup_mean = round(speedup_mean, 2),
mean_abs_rel_bias_IC = mean_abs_bias_ic,
mean_abs_rel_bias_flexIC = mean_abs_bias_flexic,
var_abs_rel_bias_IC = var_abs_bias_ic,
var_abs_rel_bias_flexIC = var_abs_bias_flexic
)
}
}
}
do.call(rbind, results)
}
# ---------- 3. Run It ----------
# Define grids
n_values <- c(200, 500, 1000)
k_values <- c(2, 5, 10)
rho_conditions <- list(low = 0.3, medium = 0.65, high = 0.9)
# Run full crossed simulation
benchmark_crossed_results_with_rho <- full_crossed_benchmark_bias_rho(n_values, k_values, rho_conditions, reps = 10, eps = "none")
# View results
print(benchmark_crossed_results_with_rho)
# ===============================================================
# fastIC_eps_benchmark.R
# IC  : deterministic Iman–Conover permutation
# Stoch: single-draw accept-until-ε (MVN draw + ε check)
# Grid: n × k × eps   •   500 reps per cell
# ===============================================================
library(MASS)           # mvrnorm()
library(microbenchmark)
## --- 1.  Algorithms ---------------------------------------------
ic_exact <- function(n, Sigma) {
k  <- ncol(Sigma)
Z  <- mvrnorm(n, rep(0, k), Sigma)
R  <- apply(Z, 2, rank, ties.method = "first")
Zs <- apply(Z, 2, sort)
X  <- matrix(NA_real_, n, k)
for (j in seq_len(k)) X[R[, j], j] <- Zs[, j]
X
}
stochIC <- function(n, Sigma, eps = 0.04, max_try = 20) {
target_S <- (6 / pi) * asin(Sigma[1, 2] / 2)
tries <- 0
repeat {
tries <- tries + 1
Z   <- mvrnorm(n, rep(0, ncol(Sigma)), Sigma)
err <- max(abs(cor(Z, method = "spearman")[upper.tri(Sigma)] - target_S))
if (err < eps || tries >= max_try) {
attr(Z, "tries") <- tries
attr(Z, "err")   <- err
return(Z)
}
}
}
## --- 2.  Design grid --------------------------------------------
n_vec   <- c(200, 500, 1000, 5000)
k_vec   <- c(2, 5, 10)
eps_vec <- c(0.02, 0.04, 0.06)
rho     <- 0.65
reps    <- 1000
grid <- expand.grid(n = n_vec, k = k_vec, eps = eps_vec,
KEEP.OUT.ATTRS = FALSE)
set.seed(42)            # reproducible redraw counts
run_cell <- function(n, k, eps) {
Sigma <- matrix(rho, k, k); diag(Sigma) <- 1
mb <- microbenchmark(
IC    = ic_exact(n, Sigma),
Stoch = stochIC(n, Sigma, eps = eps),
times = reps, unit = "ms"
)
med_ic    <- median(mb$time[mb$expr == "IC"])    / 1e6
med_st    <- median(mb$time[mb$expr == "Stoch"]) / 1e6
speed_up  <- round(med_ic / med_st, 2)
last <- stochIC(n, Sigma, eps = eps)
data.frame(
n, k, eps,
med_ms_IC    = round(med_ic,    2),
med_ms_Stoch = round(med_st,    2),
speed_up,
redraws      = attr(last, "tries"),
max_delta    = round(attr(last, "err"), 4)
)
}
results <- do.call(
rbind,
Map(run_cell, grid$n, grid$k, grid$eps)
)
write.csv(results, "E:/06_NNCORx/fastIC_eps_benchx.csv", row.names = FALSE)
print(results)
## (optional) save raw timing objects
# saveRDS(results, "fastIC_eps_raw.rds")
res <- read.table("clipboard", header = TRUE)  # or df <- your data frame
head(clipboard)
res <- read.table("clipboard", header = TRUE)  # or df <- your data frame
# pick IC baseline as eps = max (0.06)
library(dplyr)
res <- res %>%
group_by(n, k) %>%
mutate(bias_cut = round(100 * (first(max_delta) - max_delta) / first(max_delta), 0)) %>%
ungroup()
pubtab <- res %>%
mutate(
IC_ms    = round(med_ms_IC, 2),
FLEX_ms  = round(med_ms_Stoch, 2),
speed    = round(speed_up, 2),
maxDelta = sprintf("%.4f", max_delta),
BiasCut  = paste0("↓", bias_cut, "%")
) %>%
select(n, k, eps, IC_ms, FLEX_ms, speed, redraws, maxDelta, BiasCut)
print(pubtab, row.names = FALSE)
print(pubtab, row.names = FALSE, n=36)
install.packages("D:/crglm_0.1.0.tar.gz")
library(crglm)
set.seed(42)
n_groups <- 15
n_per_group <- 30
n <- n_groups * n_per_group
p <- 10
group <- factor(rep(1:n_groups, each = n_per_group))
u <- rnorm(n_groups, 0, 0.7)
X <- matrix(rnorm(n * p), nrow = n)
colnames(X) <- paste0("X", 1:p)
eta <- X %*% rep(0.2, p) + u[group]
prob <- 1 / (1 + exp(-eta))
y <- rbinom(n, 1, prob)
dat <- data.frame(y = y, group = group, X)
form <- as.formula(paste0("y ~ ", paste0("X", 1:p, collapse = " + ")))
fit <- crglm(formula = form, data = dat, group = "group", lambda = 1, rho = 1)
cat("fit$u length:", length(fit$u), "\n")
print(fit$u)
Xmat <- model.matrix(form, data = dat)
eta_hat <- Xmat %*% fit$beta + fit$u[as.integer(dat$group)]
prob_hat <- 1 / (1 + exp(-eta_hat))
# Check
cat("Prediction check:\n")
print(summary(prob_hat))
cat("Accuracy:\n")
print(mean((prob_hat > 0.5) == y))
# crglm.R - Corrected CRGLM wrapper with random intercept output
crglm <- function(formula, data, group, lambda = 1, rho = 1, family = "binomial") {
stopifnot(family == "binomial")
# Build model frame
mf <- model.frame(formula, data)
y <- model.response(mf)
X <- model.matrix(attr(mf, "terms"), data = mf)
group_factor <- factor(data[[group]])
group_index <- as.integer(group_factor) - 1  # TMB uses 0-based indexing
# Load compiled model
dll_path <- paste0(system.file("libs", .Platform$r_arch, package = "crglm"), "/crglm.dll")
dyn.load(dll_path)
# Data passed to TMB
data_list <- list(
y = as.numeric(y),
X = X,
group = group_index,
lambda = lambda,
rho = rho
)
parameters <- list(
beta = rep(0, ncol(X)),
u = rep(0, length(unique(group_index))),
log_sigma_u = 0
)
obj <- TMB::MakeADFun(
data = data_list,
parameters = parameters,
random = "u",
DLL = "crglm",
silent = TRUE
)
opt <- stats::nlminb(start = obj$par, objective = obj$fn, gradient = obj$gr)
structure(list(
beta = opt$par[grep("beta", names(opt$par))],
log_sigma_u = opt$par["log_sigma_u"],
u = obj$env$last.par.random,
convergence = opt$convergence,
objective = opt$objective,
call = match.call()
), class = "crglm")
}
fit$u
# crglm.R - Corrected CRGLM wrapper with random intercept output
crglm <- function(formula, data, group, lambda = 1, rho = 1, family = "binomial") {
stopifnot(family == "binomial")
# Build model frame
mf <- model.frame(formula, data)
y <- model.response(mf)
X <- model.matrix(attr(mf, "terms"), data = mf)
group_factor <- factor(data[[group]])
group_index <- as.integer(group_factor) - 1  # TMB uses 0-based indexing
# Load compiled model
dll_path <- paste0(system.file("libs", .Platform$r_arch, package = "crglm"), "/crglm.dll")
dyn.load(dll_path)
# Data passed to TMB
data_list <- list(
y = as.numeric(y),
X = X,
group = group_index,
lambda = lambda,
rho = rho
)
parameters <- list(
beta = rep(0, ncol(X)),
u = rep(0, length(unique(group_index))),
log_sigma_u = 0
)
obj <- TMB::MakeADFun(
data = data_list,
parameters = parameters,
random = "u",
DLL = "crglm",
silent = TRUE
)
opt <- stats::nlminb(start = obj$par, objective = obj$fn, gradient = obj$gr)
structure(list(
beta = opt$par[grep("beta", names(opt$par))],
log_sigma_u = opt$par["log_sigma_u"],
u = obj$env$last.par.random,
convergence = opt$convergence,
objective = opt$objective,
call = match.call()
), class = "crglm")
}
library(crglm)
set.seed(123)
n_groups <- 15
n_per_group <- 30
n <- n_groups * n_per_group
p <- 10
group <- factor(rep(1:n_groups, each = n_per_group))
u <- rnorm(n_groups, 0, 0.7)
X <- matrix(rnorm(n * p), nrow = n)
colnames(X) <- paste0("X", 1:p)
eta <- X %*% rep(0.2, p) + u[group]
prob <- 1 / (1 + exp(-eta))
y <- rbinom(n, 1, prob)
dat <- data.frame(y = y, group = group, X)
form <- as.formula(paste0("y ~ ", paste0("X", 1:p, collapse = " + ")))
fit <- crglm(formula = form, data = dat, group = "group", lambda = 1, rho = 1)
cat("fit$u length:", length(fit$u), "\n")
print(summary(fit$u))
sem(model = model_string, data = dmc_dat, estimator = "MLR", orthogonal = TRUE)
# LMSsim2_FINAL.R — Complete Working Version
# Kevin E. Wells — May 25, 2025
# ─────────────────────────────────────────────
# SECTION 1: Setup
# ─────────────────────────────────────────────
base_dir <- "D:/LI"
reps_per_condition <- 2
conditions <- read.csv(file.path(base_dir, "conditions.csv"), stringsAsFactors = FALSE)
library(MASS)
library(MplusAutomation)
library(lavaan)
library(semTools)
x1_vars <- paste0("x1_", 1:3)
x2_vars <- paste0("x2_", 1:3)
y_vars  <- paste0("y_",  1:3)
residualCentering <- function(data, indicators.X, indicators.M) {
for (x in indicators.X) {
for (m in indicators.M) {
newname <- paste0(x, "_RC_", m)
res <- lm(data[[x]] ~ data[[m]])$residuals
data[[newname]] <- scale(res, scale = FALSE)
}
}
return(data)
}
doubleMeanCenter <- function(data, indicators) {
X <- indicators[[1]]
M <- indicators[[2]]
for (x in X) data[[x]] <- scale(data[[x]], scale = FALSE)
for (m in M) data[[m]] <- scale(data[[m]], scale = FALSE)
for (x in X) {
for (m in M) {
int_name <- paste0(x, "_DMC_", m)
data[[int_name]] <- data[[x]] * data[[m]]
}
}
return(data)
}
# ─────────────────────────────────────────────
# SECTION 2: Data Generation
# ─────────────────────────────────────────────
simulate_data <- function(n, rho, meas_sd, loading_type) {
Sigma <- matrix(c(1, rho, 0, rho, 1, 0, 0, 0, 1), 3, 3)
latent <- mvrnorm(n, mu = c(0, 0, 0), Sigma)
colnames(latent) <- c("X1", "X2", "Y_lat")
latent[, "Y_lat"] <- 0.4 * latent[, "X1"] + 0.4 * latent[, "X2"] + 0.3 * latent[, "X1"] * latent[, "X2"] + rnorm(n, 0, 0.5)
if (loading_type == "uniform") {
load <- rep(0.6, 3)
} else if (loading_type == "narrow") {
load <- c(0.5, 0.6, 0.7)
} else if (loading_type == "wide") {
load <- c(0.4, 0.6, 0.8)
} else stop("Invalid loading_type")
x1 <- sweep(matrix(latent[, "X1"], n, 3), 2, load, "*") + matrix(rnorm(n * 3, 0, meas_sd), n)
x2 <- sweep(matrix(latent[, "X2"], n, 3), 2, load, "*") + matrix(rnorm(n * 3, 0, meas_sd), n)
y  <- sweep(matrix(latent[, "Y_lat"], n, 3), 2, load, "*") + matrix(rnorm(n * 3, 0, meas_sd), n)
colnames(x1) <- x1_vars
colnames(x2) <- x2_vars
colnames(y)  <- y_vars
return(data.frame(x1, x2, y))
}
for (i in 1:nrow(conditions)) {
cond <- conditions[i, ]
cond_folder <- file.path(base_dir, paste0("cond", cond$cond))
dir.create(cond_folder, recursive = TRUE, showWarnings = FALSE)
write.csv(cond, file = file.path(cond_folder, "design.csv"), row.names = FALSE)
for (r in 1:reps_per_condition) {
seed <- as.integer(paste0(cond$cond, sprintf("%03d", r)))
set.seed(seed)
dat <- simulate_data(cond$n, cond$rho, cond$meas_sd, cond$loading_type)
write.table(dat, file = file.path(cond_folder, sprintf("rep_%04d.dat", r)), row.names = FALSE, col.names = FALSE)
}
}
# ─────────────────────────────────────────────
# SECTION 3: Mplus Syntax Generation
# ─────────────────────────────────────────────
write_mplus_input <- function(var_names, inp_path, dat_name) {
lines <- c(
paste0("TITLE: ", basename(inp_path), ";"),
paste0("DATA: FILE = ", dat_name, ";"),
"VARIABLE:",
paste0("  NAMES = ", paste(var_names, collapse = " "), ";"),
paste0("  USEVARIABLES = ", paste(var_names, collapse = " "), ";"),
"MODEL:",
"  X1 BY x1_1 x1_2 x1_3;",
"  X2 BY x2_1 x2_2 x2_3;",
"  Y  BY y_1  y_2  y_3;",
"  X1 WITH X2;",
"  XINT | X1 XWITH X2;",
"  Y ON X1 X2 XINT;",
"ANALYSIS:",
"  TYPE = RANDOM;",
"  ESTIMATOR = MLR;",
"  ALGORITHM = INTEGRATION;",
"OUTPUT: NOCHISQUARE;"
)
writeLines(lines, con = inp_path)
}
for (i in 1:nrow(conditions)) {
cond_folder <- file.path(base_dir, paste0("cond", conditions$cond[i]))
for (r in 1:reps_per_condition) {
dat_file <- sprintf("rep_%04d.dat", r)
inp_file <- sprintf("rep_%04d.inp", r)
if (file.exists(file.path(cond_folder, dat_file))) {
write_mplus_input(c(x1_vars, x2_vars, y_vars),
file.path(cond_folder, inp_file), dat_file)
}
}
}
# ─────────────────────────────────────────────
# SECTION 4: Run Mplus
# ─────────────────────────────────────────────
for (i in 1:nrow(conditions)) {
cond_folder <- file.path(base_dir, paste0("cond", conditions$cond[i]))
runModels(target = cond_folder, recursive = FALSE)
}
base_dir <- "C:/LI"
reps_per_condition <- 2
conditions <- read.csv(file.path(base_dir, "conditions.csv"), stringsAsFactors = FALSE)
library(MASS)
library(MplusAutomation)
library(lavaan)
library(semTools)
bias_sim <- read.csv(file.path(base_dir, "bias_sim.csv"))
bias_lms <- read.csv(file.path(base_dir, "bias_lms.csv"))
summarize_bias <- function(df) {
group_cols <- c("cond", "n", "rho", "meas_sd", "loading_type")
bias_cols <- setdiff(names(df), group_cols)
bias_cols <- bias_cols[sapply(df[bias_cols], is.numeric)]
df_subset <- df[, c(group_cols, bias_cols)]
# Compute mean and sd for each bias column within condition
df_mean <- aggregate(. ~ cond + n + rho + meas_sd + loading_type, data = df_subset, FUN = function(x) mean(x, na.rm = TRUE))
df_sd   <- aggregate(. ~ cond + n + rho + meas_sd + loading_type, data = df_subset, FUN = function(x) sd(x, na.rm = TRUE))
# Rename columns
names(df_mean)[-1:-(length(group_cols))] <- paste0(names(df_mean)[-1:-(length(group_cols))], "_mean")
names(df_sd)[-1:-(length(group_cols))]   <- paste0(names(df_sd)[-1:-(length(group_cols))], "_sd")
# Merge mean and sd tables
merge(df_mean, df_sd, by = group_cols)
}
write.csv(summarize_bias(bias_sim), file.path(base_dir, "bias_sim_summary.csv"), row.names = FALSE)
write.csv(summarize_bias(bias_lms), file.path(base_dir, "bias_lms_summary.csv"), row.names = FALSE)
cat("✔ Flat summary tables saved. Section 9 complete.\n")
base_dir
# ----- LOAD SAVED CSVs ----------------------------------------------------
EFAna <- restore_ord(read.csv(paste0(csv_path, "EFA_half_wave5.csv"),
stringsAsFactors = FALSE))
first_run <- FALSE                        # TRUE first run only
sav_file  <- "E:/21_NQ/ECLSK.sav"                 # <<< path to stitched .sav
csv_path  <- "E:/21_NQ/"                          # <<< folder for all CSVs
weight    <- "C5CW0"                              # wave‑5 child weight
repwts    <- paste0("C5CRW", sprintf("%02d", 1:80))
library(haven)      # read_sav
library(polycor)    # hetcor
library(psych)      # KMO, fa
library(lavaan)     # CFA / SEM
library(semTools)   # reliability()
manifest <- c("PLAY","TRASH","DRUGS","BURG","VIOLENT","VACANT",
"PARED","INCOME","PARPRES")
outcomes <- c("MACH","RACH")
ordinals <- c("PLAY","TRASH","DRUGS","BURG","VIOLENT","VACANT","GENDER")
bad <- c(-9,-8,-7,-1)          # special missing codes
restore_ord <- function(df) {
df[ordinals] <- lapply(df[ordinals], ordered)
df
}
if (first_run) {
# ----- 1. READ & CLEAN ---------------------------------------------------
dat <- read_sav(sav_file)
dat[] <- lapply(dat, function(x)
if (inherits(x,"haven_labelled")) zap_labels(x) else x)
# wave‑5 variable mapping
dat$PLAY    <- dat$P5SAFEPL
dat$TRASH   <- dat$P5GARBAG
dat$DRUGS   <- dat$P5DRUG
dat$BURG    <- dat$P5BURGLR
dat$VIOLENT <- dat$P5VIOLEN
dat$VACANT  <- dat$P5VACANT
dat$PARED   <- dat$W5PARED
dat$INCOME  <- dat$W5INCCAT
dat$PARPRES <- pmax(dat$W5DADSCR, dat$W5MOMSCR, na.rm = TRUE)
dat$MACH    <- dat$C5R4MSCL
dat$RACH    <- dat$C5R4RSCL
dat$GENDER  <- ordered(dat$GENDER)
# recode special codes to NA
dat[ordinals] <- lapply(dat[ordinals],
function(x) replace(x, x %in% bad, NA))
dat[c("PARED","INCOME","PARPRES")] <-
lapply(dat[c("PARED","INCOME","PARPRES")],
function(x) replace(x, x %in% bad, NA))
need <- c(manifest, outcomes, "GENDER", weight)
dat  <- dat[complete.cases(dat[ , need]), ]
dat  <- restore_ord(dat)
write.csv(dat,  paste0(csv_path, "clean_ECLSK_wave5.csv"), row.names = FALSE)
# ----- 2. 50/50 SPLIT ----------------------------------------------------
set.seed(101)
idx   <- sample.int(nrow(dat), floor(.5*nrow(dat)))
EFAna <- dat[idx, ]
CFAna <- dat[-idx, ]
write.csv(EFAna, paste0(csv_path, "EFA_half_wave5.csv"), row.names = FALSE)
write.csv(CFAna, paste0(csv_path, "CFA_half_wave5.csv"), row.names = FALSE)
} else {
# ----- LOAD SAVED CSVs ----------------------------------------------------
EFAna <- restore_ord(read.csv(paste0(csv_path, "EFA_half_wave5.csv"),
stringsAsFactors = FALSE))
CFAna <- restore_ord(read.csv(paste0(csv_path, "CFA_half_wave5.csv"),
stringsAsFactors = FALSE))
}
cat("EFA n =", nrow(EFAna), " | CFA n =", nrow(CFAna), "\n")
describe(EFA)
describe(EFAna)
setwd("D:/SNC_0.1.0")
devtools::document()
devtools::document()
